1 PRED: Trainable Methods For Surface Natural Language Generation We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain. This paper presents three trainable systems for surface natural language generation (NLG). Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation. The systems take a "corpus-based" or "machinelearning" approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations. The annotation scheme used a total of 26 attributes to represent flights. The training and test sets used to evaluate NLG1, NLG2, and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain. The annotation scheme used a total of 26 attributes to represent flights. The annotation scheme used a total of 26 attributes to represent flights. The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain. The annotation scheme used a total of 26 attributes to represent flights. The annotation scheme used a total of 26 attributes to represent flights. The annotation scheme used a total of 26 attributes to represent flights. The training and test sets used to evaluate NLG1 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain.


2 PRED: Applied Text Generation This paper presents the Joyce system as an example of a fully-implemented, application-oriented text generation system. Joyce covers the whole range of tasks associated with text generation, from content selection to morphological processing. It was developped as part of the interface of the software design environment Ulysses. The following design goals were set for it: While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Ra.mbow 1990). This paper will present the system and attempt to show how these design objectives led to particular design decisions. We propose a graphical interface for the Joyce system.


3 PRED: Three Heads Are Better Than One Machine-readable dictionary (the Collins Spanish/English), the lexicons used by the KBMT modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. The outputs from these engines (target language words and phrases) are recorded in a chart whose positions correspond to words in the source language input. As a result of the operation of each of the MT engines, new edges are added to the chart, each labeled with the translation of a region of the input string and indexed by this region's beginning and end positions. We will refer to all of these edges as components (as in "components of the translation") for the remainder of this article. The KBMT and EBMT engines also carry a quality score for each output element. The KBMT scores are produced based on whether any questionable heuristics were used in the source analysis or target generation. The EBMT scores are produced using a technique based on human judgements, as described in (Nirenburg et al., 1994a), submitted. We use a recursive divide-conquer algorithm to find the best possible cover from the collection of candidate partial translations by normalizing each component's quality score (positive, with larger being better), and then selecting the best combination of components with the help of the chart walk algorithm. We use a recursive divide-conquer algorithm to find the best possible subwalks.


4 PRED: Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study tile elfeels of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classiicr, and establish that adjectives on a simple subjectivity classiicr, and establish that adjectives on a simple subjectivity classiicr, and establish that adjectives on a simple subjectivity classiicr, and establish strong predictors of subjectivity. A novel train-able method that Thai statistically combines two indicators of gradability is presented and ewlhlalcd, complementing automatic techniques for assigning orientation labels. We use a lexical lexical lexicon to learn adjectives from corpora. We use a lexicon to learn adjectives from corpora to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a lexicon to learn a gradable orientation label. We use a gradable adjective classifier to classify the subjectivity classifier.


5 PRED: Statisti(:a,1 signiticance testing of (litleren) in v(:s of metri(:s like recall, i)rccision and bat-(:(l) F-s(:()rc is a necessary t)art of eml)irical ual(ural language 1)ro(:essing. Unfortunately, we lind in a set of (;Xl)erinlc]d;s (;hal; many (:ore-inertly used tesl;s so are less likely to detect differences that exist 1)el;ween ditlercnt techniques. This undelesiimation comes from an inlet-endcn( a,-;SUlnl)tion that is often violated. fe l)oint out some useful l;e,%s (;hal; (lo nol; make this assumllion, including computationally--intcnsive rand()mizat,ion 1;cs|;s. We show that the new technique is better than the old technique. We show that the results on the test set are better than the old technique. We show that the results on the MITR1 are better than the old technique. We show that the results on the MITR1 are better than the old technique.


6 PRED: A Graph Model For Unsupervised Lexical Acquisition This paper presents an unsupervised method forassembling semantic knowledge from a part-of- speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relations. We focus on the symmetric relationship between pairs of nouns which occur to gether in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific aspects as distinct components in the graph surrounding an ambiguous word. We use a graph model to extract similar words from a part-of-speech tagged corpus. We use a graph model to extract similar words from a part-of-speech corpus.


7 PRED: Building A Large-Scale Annotated Chinese Corpus In this paper we address issues related to building a large-scale Chinese corpus. We try to answer four questions: (i) how to speed up annotation, (ii) how to maintain high annotation quality, (iii) for what purposes is the corpus applicable, and finally (iv) what future work we anticipate. We use a Chinese Treebank to build a large-scale Chinese corpus. We use a linguistically robust set of guidelines for annotation accuracy and usability.


8 PRED: Wide-Coverage Semantic Representations From A CCG Parser This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic inference. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP. We use a combination of a lexicalised grammar and a combinatory grammar to generate semantic representations. We use a combination of a lexicalised grammar and a small number of semantically transparent combinatory rules to combine CCG categories. We use a lexicalised grammar to generate semantic representations for a wide-coverage statistical parser.


9 PRED: A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English In this paper, we present an approach to the automatic identification and correction of preposition and determiner errors in non native (L2) English writing. We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. We use the Cambridge Learner Corpus (CLC) to identify prepositions and determiner errors in English and show that the classifier can be trained with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. We use a classifier-based approach to the automatic identification and correction of preposition and determiner errors in non native (L2) English writing. We use a classifier-based approach to the acquisition of models of use for these parts of speech, which achieves an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing. We use a classifier-based approach to the acquisition of preposition and determiner errors in English, which is based on a contextual feature-based approach to the acquisition of preposition and determiner errors in English, and a classifier-based approach to the acquisition of preposition and determiner errors in English, and a classifier-based approach to the acquisition of preposition and determiner-based model. We use a sub-section of the Cambridge Learner Corpus (CLC) as a model to learn prepositions and determiner-based model to learn prepositions in English, and then use a classifier-based model to learn prepositions in English, and then use a classifier-based model to learn prepositions in English.


10 PRED: A Monolingual Tree-based Translation Model for Sentence Simplification In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target. We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. We propose a tree-based model for sentence simplification, which is based on tree transformation. We propose a tree-based model for sentence simplification, which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. We propose a tree-based model for sentence simplification, which is based on tree transformation.


11 PRED: Categorial Unification Grammars Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms. Their efficient and uniform way of encoding linguistic knowledge in well-understood and widely used representations makes them attractive for computational applications and for linguistic research. In this paper, the basic concepts of CUGs and simple examples of their application will be presented. It will be argued that the strategies and potentials of CUGs justify their further exploration in the wider context of research on unification grammars. Approaches to selected linguistic phenomena such as long-distance dependencies, adjuncts, word order, and extraposition are discussed. The work on merging strategies from unification grammars and categorial grammars has its origins in several research efforst that have been pursued in parallel. One of them is the grammar development on the PATR system (Shieber et al, 1983; Shieber, 1984) at SRI. For quite a while now I have been using the excellent facilities of PATR for the design and testing of experimental CUGs. Such grammars currently run on two PATR implementations: Stuart Shieber's Zetalisp version on the Symbolics 3600 and Lauri Karttunen's Interlisp-D version on the XEROX 1109. The work on CUGs has influenced our efforts to develop a larger PATR grammar, and will do so even more in the future. The work on CUGs has influenced our efforts to develop a larger PATR grammar, and will do so even more in the future. The work on CUGs has influenced our efforts to develop a larger PATR grammar, and will do so even more in the future. The work on CUGs has influenced our efforts to develop a larger PATR grammar, and will do so even more in the future. We propose a linguistically motivated CUG framework with a strong lexical syntax that accomodates word order variation. We propose a linguistically motivated CUG framework with a strong lexical syntax that accomodates word order variation.


12 PRED: A Uniform Architecture for Parsing and Generation The use of a single grammar for both parsing and generation is an idea with a certain elegance, the desirability of which several researchers have noted. In this paper, we discuss a more radical possibility: not only can a single grammar be used by different processes engaged in various "directions" of processing, but one and the same language-processing architecture can be used for processing the grammar in the various modes. In particular, parsing and generation can be viewed as two processes engaged in by a single parameterized theorem prover for the logical interpretation of the formalism. We discuss our current implementation f such an architecture, which is parameterized in such a way that it can be used for either purpose with grammars written in the PATR formal- ism. Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes. We propose a unified architecture for parsing and generation that is parameterized in such a way that it can be used either for either purpose with respect to grammars written in a particular formalism which has a logical semantics, the PATR formal-ism. Furthermore, the architecture allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes. We propose a uniform architecture for parsing and generation, which is parameterized in such a way that it can be used either for parsing and generation, and allows fine tuning to reflect different processing strategies, including parsing models intended to mimic psycholinguistic phenomena. This tuning allows the parsing system to operate within the same realm of efficiency as previous architectures for parsing alone, but with much greater flexibility for engaging in other processing regimes. We propose a uniform architecture for parsing and generation, which is parameterized in such a way that it can be used either for either purpose with respect to grammars written in a particular formalism written in


13 PRED: Synchronous Tree-Adjoining Grammars The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language. We present a variant of "FAGs, called synchronous TAGs, which characterize correspondences between languages. The formalism's intended usage is to relate expressions of natural anguages to their associated semantics represented in a logical form language, or to their translations in another natural language; in sum, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mention primarily in passing some computational issues that arise in its interpretation. We propose a method for describing the semantics of a sentence, which is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that is a formalism that


14 PRED: Stochastic Lexicalized Tree-Adjoining Grammars - An extension to context free grammars. Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech communication papers presentacd at the 97th Meeting of the Acoustical Society of Australia, MIT, Cambridge, MA, June. Booth, T. 1969. Probabilistic representation of formal languages. In Tenth Annual IEEE Symposium on Switching and Automata Theory, October. Syntactic Structures, chapter 2-3, pages 13-18. Applicability of indexed grammars to natural anguages. Technical Report CSLI-85-34, Center for Study of Language and Information. The ATIS spoken language system system pilot corpus. In DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, June. We present a stochastic Lexicalized Tree-Adjoining Grammar. We present a stochastic Lexicalized Tree-Adjoining Grammar. We present a stochastic tree-adjoining Grammar.


15 PRED: Anaphora For Everyone: Pronominal Anaphora Resolution Without A Parser We present an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass,/994). In contrast to that work, our algorithm does not require in-depth, full, syntactic parsing. Instead, with minimal compromise in output quality, the modifications enable the resolution process to work from tile output of a part of speech tagger, enriched only with annotations of grammatical functions of lexical items in the in- put text stream. Evaluation of the results of our complementation demonstrates that ac curate anaphora resolution can be realized within natural language processing works which do not--r cannot- employ ro- bust and rcqiable parsing components. We propose an algorithm for anaphora resolution which is a modified and extended version of that developed by (Lappin and Leass, 1994).


16 PRED: MUC Evaluations We have just completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several new innovations over prior MUCs, most notably in the range of different asks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.


17 PRED: Polylingual Topic Models Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. We propose a topic model that is a direct translation model for document tuples.


18 PRED: A Multi-Pass Sieve for Coreference Resolution Most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features. This approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones. To overcome this problem, we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output. Further, our model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster. This cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time. The framework is highly modular: new coreference modules can be plugged in without any change to the other modules. In spite of its simplicity, our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora. This suggests that sieve-based approaches could be applied to other NLP tasks. We propose a multi-pass sieve-like approach to coreference resolution that applies tiers of deterministic coreference models one at a time from highest to lowest precision. Each tier builds on the previous tier's entity cluster output.


19 PRED: Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning. Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method. The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences. We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model. Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations. We use a log-linear CCG grammar to learn a logical form model. We use a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on a lexicon based on


20 PRED: Multi-Source Transfer of Delexicalized Dependency Parsers We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data. We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers. We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-the-art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. We use a delexicalized parser to generate dependency parsers for a target language.


21 PRED: Named Entity Recognition in Tweets: An Experimental Study People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to recognition. Our novel doubles compared with the NER system. The redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms coincreasing 25% over ten common entity types. We propose a distancely supervised approach to classify named entities in tweets. We use a dataset of previously-tagged out-of-domain text, 2 tagged tweets, and unlabeled tweets to learn the lexical structure of a given entity.


22 PRED: Identifying Relations for Open Information Extraction Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We introduce the constraints in the Open IE system, which more than doubles the area under the precision-recall curve relative previous extractors such as More than are at precision higher compared to virtually none for earlier systems. The paper concludes with a detailed analysis. We introduce a constraint that allows the extraction of arbitrary relations from English sentences, obviating the restriction to a pre-specified vocabulary. We introduce a constraint that allows the extraction of arbitrary relations from English sentences, obviating the restriction to a pre-specified vocabulary.


23 PRED: An Efficient Implementation Of A New DOP Model Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFG reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence. We propose a parser for a parser, which uses a parser to compute the parse tree involving the most frequent subtrees from a treebank.


24 PRED: Combining Distributional And Morphological Information For Part Of Speech Induction In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. We use a morphological clustering algorithm to cluster words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages. We use a morphological clustering algorithm to cluster words into classes from unlabelled text using unsupervised algorithms, based on a restricted form into a distributional clustering algorithm. We use a morphological information restricted form into a distributional clustering algorithm. We use a lexical clustering algorithm to cluster words into classes from unlabelled text.


25 PRED: Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations We investigate the lexical and syntactic flexibility of a class of idiomatic expressions. We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. We propose a method for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. We propose a method for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation. We propose a method for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation.


26 PRED: Named Entity Recognition Without Gazetteers It is often claimed that Named Entity recognition systems need extensive gazetteers—lists of names of people, organisations, locations, and other named entities. Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems. We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models. We report on the system's performance with gazetteers of different types and different sizes, using test material from the muc-7 competition. We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names. We conclude with observations about the domain independence of the competition and of our experiments. We use a list of names of people, organisations and locations to identify names of people, organisations and locations.


27 PRED: Extracting Product Features And Opinions From Reviews Consumers are often forced to wade through many on-line reviews in order to make an informed decision. This paper introduces OPINE, an unsupervised information extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE's novel use of relaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity. We use a relaxation labeled opinion phrase extraction model to extract opinion phrases from reviews.


28 PRED: Non-Projective Dependency Parsing Using Spanning Tree Algorithms We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al, 2003; McDonald et al, 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. We use a lexical dependency grammar to parse a dependency grammar based on a lexical dependency grammar.


29 PRED: Emotions From Text: Machine Learning For Text-Based Emotion Prediction In addition to information, text contains attitudinal, and more specifically, emotional content. This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture. The goal is to classify the emotional affinity of sentences in the narrative domain of children's fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis. Initial experiments on a preliminarydata set of 22 fairy tales show encouraging results over a neutral baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning. We also discuss results for a tripartite model which covers emotional valence, as well as feature set acquisitions. In addition, we present plans for a more cognitively soundequential model, taking into account a larger set of basic emotions. Text does not only communicate informative conditions, but also attitudinal information, including emotional states. We propose a model for text-based emotion prediction, which uses supervised machine learning to learn the emotional content of sentences.


30 PRED: A Maximum Entropy Model for Prepositional Phrase Attachment A parser for natural language must often choose between two or more equally grammatical parses for the same sentence. Often the correct parse can be determined from the lexical properties of certain key words or from the context in which the sentence occurs. For example in the sentence, In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos. The prepositional phrase on virtually all uses of asbestos can attach to either the noun phrase a gradual ban [vP imposed [JvP a gradual ban [pp on virtually all uses of asbestos]] or the verb phrase imposed, yielding [vP imposed [uP a gradual ban][iop on virtually all uses off asbestos]]. For this example, a human annotators attachment decision, which for our purposes is the "correct" attachment, is to the noun phrase. We present in this paper methods for constructioning statistical models for computing the probability of attachment decisions. These models could be then integrated into scoring the probability of an overall parse. These search strategies can be applied to other attachment decisions. We use a maximum entropy model to model the distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution. We use a maximum entropy model to model the distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution. We use a maximum entropy model to model the distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution of the binary distribution. We use the IBM-Lancaster Treebank of Computer Manuals and the University of Pennsylvania WSJ treebank.


31 PRED: Learning Dependency Translation Models As Collections Of Finite-State Head Transducers The paper defines weighted head transducers, finite-state machines that perform middle-out string transduction. These transducers are strictly more expressive than the special case of standard left-to-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. We define a dependency transduction model in terms of a collection of weighted head transducers. Each head transducer is a finite-state machine that differs from "standard" finite-state transducers in that, instead of consuming the input string left to right, it consumes it "middle out" from a symbol in the string. We define a dependency transduction model in terms of a collection of weighted head transducers. We define a dependency transduction model in terms of a collection of weighted head transducers. Each head transducer is a finite-state machine that differs from "standard" finite-state transducers in that, instead of consuming the input string "middle out" from a symbol in the string. We define a dependency transduction model in terms of a collection of weighted head transducers. We define a dependency transduction model in terms of a collection of weighted head transducers.


32 PRED: An Empirically Based System For Processing Definite Descriptions We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions. We propose a method for identifying bridging descriptions. We propose a method for identifying bridging and direct anaphora.


33 PRED: On Coreferring: Coreference In MUC And Related Annotation Schemes In this paper, it is argued that "coreference" annotations, as performed in the MUC community for example, go well beyond annotation of the relation of coreference proper. As a result, it is not always clear what semantic relation these annotations are encoding. The paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded. In particular, it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative NP. Various practical tasks requiring language technology including, for example, information extraction and text summarization, can be performed more reliably if it is possible to automatically find parts of the text containing information about a given topic. For example, if a text summarizer has to select the most important information, then the summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash. For example, if a text summarizer has to select the most important information, then the summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash. For example, if a text summarizer has to select the most important information, in a given text summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash. For example, if a text summarizer has to select the most important information, then the summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash, then the summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash. For example, if a program can automatically spot all the clauses in the text that contain information about the 1984 Wall Street crash, then the summarization task is greatly helped if a program can automatically spot all the clauses in the text that contain information about this crash. In this paper, it is argued that "coreference" annotations


34 PRED: Probabilistic Top-Down Parsing And Language Modeling This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model. We introduce a lexicalized probabilistic top-down parser, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing model, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model. We introduce probabilistic context-free grammars, and introduce a new language model that utilizes probabilistic top-down parsing, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model.


35 PRED: Using The Web To Obtain Frequencies For Unseen Bigrams This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus. We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine. We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task. We use the Web to obtain the frequency of a bigram by querying a search engine.


36 PRED: The Importance of Syntactic Parsing and Inference in Semantic Role Labeling We present a general framework for semantic role labeling. The framework combines a machine-learning technique with an integer linear programming?based inference procedure, which incorporates linguistic and structural constraints into a global decision process. Within this framework, we study the role of syntactic parsing information in semantic role labeling. We show that full syntactic parsing information is, by far, most relevant in identifying the argument, especially, in the very first stage - the pruning stage. Surprisingly, the quality of the pruning stage cannot be solely determined based on its recall and precision. Instead, it depends on the characteristics of the output candidates that determine the difficulty of the downstream prob- ms. Motivated by this observation, we propose an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves its performance. Our system has been evaluated in the CoNLL-2005 shared task on semantic role labeling, and achieves the highest F1 score among 19 participants. We use a supervised semantic parser to learn the semantic role labeling tree.


37 PRED: Word Association Norms Mutual Information And Lexicography We present a statistical model for word association norms, based on the information theoretic concept of mutual information. We use the association ratio to measure word association norms, which is more objective and less costly than the subjective method employed in Palermo and Jenkins (1964). We use the association ratio to measure word association norms, based on the information theoretic concept of mutual information. We use the association ratio to measure word association between a word and a part of speech.


38 PRED: Lexical Cohesion Computed By Thesaural Relations As An Indicator Of The Structure Of Text In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being "about the same thing", and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences. We propose a method for building lexical chains for use as an aid in determining the structure of text. We propose a method for building a lexical chain that is based on the semantic relation between words and words.


39 PRED: Class-Based N-Gram Models Of Natural Language We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. We propose a class-based N-gram model that is based on the frequency of their co-occurrence with other words. We propose a class-based N-gram model that is based on the frequency of their co-occurrence with other words. We propose a class-based N-gram model that is based on the frequency of their co-occurrence with other words. We propose a class-based n-gram model that is based on the frequency of their co-occurrence with other words.


40 PRED: Introduction To The Special Issue On Computational Linguistics Using Large Corpora The 1990s have witnessed a resurgence of interest in 1950s-style empirical and statistical methods of language analysis. Empiricism was at its peak in the 1950s, dominating a broad set of fields ranging from psychology (behaviorism) to electrical engineering (information theory). At that time, it was common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their cooccurrence with other words. We present a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of a paper on the topic of the same paper.


41 PRED: Tagging English Text With A Probabilistic Model In this paper we present some experiments on the use of a probabilistic model to tag English text, i.e. to assign to each word the correct tag (part of speech) in the context of the sentence. The main novelty of these experiments is the use of untagged text in the training of the model. We have used a simple tri-class Markov model and are looking for the best way to estimate the parameters of this model, depending on the kind and amount of training data provided. Two approaches in particular are compared and combined: • using text that has been tagged by hand and computing relative frequency counts, • using text without tags and training the model as a hidden Markov process, according to a Maximum Likelihood principle. Experiments show that the best training is obtained by using as much tagged text as possible. They also show that Maximum Likelihood training, the procedure that is routinely used to estimate hidden Markov models parameters from training data, will not necessarily improve the tagging accuracy. In fact, it will generally degrade this accuracy, except when only a limited amount of hand-tagged text is available. We use a supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised supervised


42 PRED: A Syntactic Analysis Method Of Long Japanese Sentences Based On The Detection Of Conjunctive Structures This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. We show that the simplest approach to detecting conjunctive structures is to use a dependency analysis method that uses a dependency analysis method to detect conjunctive structures in Japanese sentences. We use a dependency analysis method to detect conjunctive structures in Japanese sentences by checking the parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures.


43 PRED: Word Sense Disambiguation Using A Second Language Monolingual Corpus This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language. This approach exploits the differences between mappings of words to senses in different languages. The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable. The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon. The preferred senses are then selected according to statistics on lexical relations in the target language. The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence. The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation. The paper includes a detailed comparative analysis of statistical sense disambiguation methods. We use a lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from a monolingual corpus of another language to choose the most appropriate alternative. We use a bilingual lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from target language corpora to choose the most appropriate alternative. We use a lexicon to find all possible translations of each tuple T to the most plausible alternative.


44 PRED: An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. We propose an Earley parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) b) c) d) c) most likely (Viterbi) c) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. We propose an Earley parser that computes the following quantities given a stochastic context-free grammar and a given input string.


45 PRED: Introduction To The Special Issue On Word Sense Disambiguation: The State Of The Art The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950s. Sense disambiguation is an "intermediate task" (Wilks and Stevenson 1996), which is not an end in itself, but rather is necessary at one level or another to accomplish most natural language processing tasks. The nineteenth century, the manual analysis of corpora has enabled the study of words and graphemes (Kaeding 1897-1898, Estoup 1902, Zipf 1935) and the extraction of lists of words and collocations for the study of language acquisition or language teaching. We propose a method for disambiguating word senses.


46 PRED: Knowledge-Free Induction Of Inflectional Morphologies We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed. We use a lexicon based on a lexicon to induce affixes and circumfixes in German, Dutch, and English. We use a lexicon based on a Latent Semantic Analysis approach to affix frequency, affix frequency, syntactic context, and transitive closure. We use a lexicon based on a knowledge-free approach to morphology induction approach to affix frequency, affix frequency, syntactic context, and transitive closure.


47 PRED: Training Tree Transducers Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling. The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to-tree and tree-to-string transducers. We introduce a class of tree transducers that performs a traversal traversal, pursuing subtrees in parallel, with each subtree transformed depending only on its own passed-down state. We introduce a class of tree transducer, which is a generalization of FST. We introduce a class of tree transducer, which is a class of tree transducer, which is a class of tree transducer, which is a class of tree transducer, which is a class of tree transducer, which is a class of tree transducer, and a class of tree transducer, and a class of tree transducer that is a class of tree transducer that is a class of tree transducer, which is a class of tree transducer, which is a class of tree transducer, and we describe the class of tree transducer, and we describe the class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a class of tree transducer that is a generalization of FST. We introduce a class of tree transducer that is a class of tree transducer that is a class of tree transducer that


48 PRED: A Unigram Orientation Model For Statistical Machine Translation The segmentation model uses a novel orientation component to handle swapping of neighbor blocks. During training, we collect block uncounts with we count how often a block occurs to the left or to the right of some predecessor block. The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model. We show experimental results on a standard Arabic-English translation task. We use a trigram language model to handle neighbor swapping. We use a trigram language model to handle swapping of neighbor blocks.


49 PRED: Automatic Tagging Of Arabic Text: From Raw Text To Base Phrase Chunks Arabic is garnering attention in the NLP community due to its socio-political importance and its linguistic differences from Indo-European languages. These linguistic characteristics, especially dialect differences and complex morphology present interesting challenges for NLP researchers. But like most non-European languages, Arabic is lacking in annotated resources and tools. Fully automated fundamental NLP tools such as Tokenizers, Part Of Speech (POS) Taggers and Base Phrase (BP) Chunkers are still not available for Arabic. Meanwhile, these tools are readily available and have achieved remarkable accuracy and sophistication for the processing of many European languages. With the release of the Arabic Penn TreeBank 1 (v2.0), the story is about to change. In this paper, we propose solutions to the problems of Tokenization, POS Tagging and BP Chunking of Arabic text. By Tokenization we mean the process of segmenting clitics from stems, since in Arabic, prepositions, conjunctions, and some pronouns are cliticized (orthographically and phonological fused) onto stems. For each of these tasks, we adopt a supervised machine learning perspective using Support Vector Machines (SVMs) trained on the Arabic TreeBank, leveraging off of already existing algorithms for English. The results are comparable to state-of-the-art results on English text when trained on similar sized data. We use a supervised machine learning approach to automatically tokenize Arabic text with a morphologically supervised machine learning perspective using Support Vector Machines (SVMs) trained on the Arabic TreeBank, leveraging off of already existing algorithms for English. The results are comparable to state-of-the-art results on English text when trained on similar sized data. We use a supervised machine learning perspective to automatically tokenize Arabic text with a lexical and semantic level.


50 PRED: Alignment By Agreement We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models. Compared to the standard practice of intersecting predictions of independently-trained models, joint training provides a 32% reduction in AER. Moreover, a simple and efficient pair of HMM aligners provides a 29% reduction in AER over symmetrized IBM model 4 predictions. We use a heuristic approximation algorithm to maximize the likelihood of agreement between the models. We use a heuristic approximation algorithm to maximize the likelihood of agreement between the models. We use a heuristic approximation algorithm to maximize the likelihood of agreement between the models. We use a heuristic approximation algorithm to maximize the likelihood of agreement between the models. We use a heuristic approximation algorithm to maximize the likelihood of agreement between the models.


51 PRED: Parser Combination By Reparsing We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers. We use a parser reparsing approach to parse constituents and dependency structures. We use a parser combination scheme to combine the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers. We use a parsing algorithm to combine the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers. This is done in a two stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage. Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set. We use a two-stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage. Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.


52 PRED: Multiple Aspect Ranking Using the Good Grief Algorithm We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreement-based joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. We propose a ranking model that is based on the agreement relation between the two.


53 PRED: Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. The many-to-many alignments result in significant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages and data sets. We use a multi-to-many aligner to find double phonemes and double letters, as opposed to manually preprocessing data by merging phonemes using fixed lists. We use a hidden Markov Model (HMM) method to predict a global phoneme sequence given a word. We use a multi-to-many alignment method to find double phonemes and double letters, as opposed to manually preprocessing data.


54 PRED: ISP: Learning Inferential Selectional Preferences Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents a collection of methods for automatically learning admissible argument values to which an inference rule be applied, which we call and methods for filtering out incorrect inferences. We present empirical evidence of its effectiveness. We propose a method for automatically learning admissible argument values for which an inference rule be applied, which we call Inferential Selectional Preferences. We propose a method for automatically learning admissible argument values for which an inference rule be applied, which we call and methods for filtering out incorrect inferences. We present empirical evidence of its effectiveness. We propose a method for automatically learning inferential selectional preferences, a collection of methods for automatically learning inference rules and filtering out incorrect inferences. We propose a method for automatically learning inferential selectional preferences.


55 PRED: Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems. Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding. For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems. We use a dependency parser to model reordering distances. We use a dependency parser to model reordering distances. We use a dependency parser to model the number of words between two source and two consecutive target phrases. We use a dependency parser to model the number of words between two sentences. We use a dependency parser to model the number of words between two sentences.


56 PRED: Linguistic Regularities in Continuous Space Word Representations Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, "King - Man + Woman" results in a vector very close to "Queen." We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. We use a recurrent neural network language model to learn word representations. We use a recurrent neural network to learn word representations.


57 PRED: Scaling To Very Very Large Corpora For Natural Language Disambiguation The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost. We show that the size of the training corpus is a significant factor in determining the correct use of a word, given a set of words, and a set of words, are often confused. We show that the size of the training corpus is a significant factor in determining the correct use of a word, and the number of words in the corpus is not as important in the training data.


58 PRED: An Unsupervised Method For Word Sense Tagging Using Parallel Corpora This paper presents an unsupervised method for word sense disambiguation that exploits translations as a proxy for semantic annotation across languages. The observation behind the approach, that words having the same translation often share some dimension of meaning, leads to an unsupervised approach that exploits translations as a proxy for semantic annotation across languages. Performance using this approach has been rigorously evaluated and is comparable with other unsupervised systems, even though it is based on cross-language lexical correspondences, a radically different source of evidence, and even though those correspondences were derived from machine translations rather than clean human translations. We propose an unsupervised approach to word sense disambiguation that uses translations as a proxy for semantic annotation across languages. We propose an unsupervised method for word sense disambiguation that uses translations as a proxy for semantic annotation across languages. We propose an unsupervised method for word sense disambiguation that uses translations as a proxy for semantic annotation across languages. We use a sense-tagged corpus to automatically tag the two languages in tandem with a single sense inventory. We use a sense-tagged corpus to automatically tag the two languages in tandem with a single sense inventory.


59 PRED: Loosely Tree-Based Alignment For Machine Translation We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. We propose a method for reordering syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length. This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms. We use a syntactic tree-to-string alignment model to generate a syntactic representation from the source language into an abstract semantic representation from which text is generated in the target language, and then reorder the tree structure of the source language into a tree structure. We use a clone operation to copy the tree structure of the source language tree into the target language sentence.


60 PRED: Accurate Unlexicalized Parsing Figure 3: Size and development performance of the cumulatively annotated models, starting with the markovized baseline. The two columns show the change in the baseline for each annotation introduced, both cumulatively and for each single annotation applied to the baseline in isolation. We show that the resulting parser was able to parse the treebanks of the Penn treebanks using the baseline. We show that the resulting parser was able to parse the treebanks of the resulting treebanks of the Penn treebanks. We show that the resulting parser is able to parse the treebanks of the Penn treebanks.


61 PRED: A Mention-Synchronous Coreference Resolution Algorithm Based On The Bell Tree This paper proposes a new approach for resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. In this paper, we will adopt the terminologies used in the Automatic Content Extraction (ACE) task (NIST, 2003). We use the Bell tree to represent the search space of the coreference resolution problem. We use the Bell tree to rank the best paths. We use the maximum entropy model to rank the best paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. We use the Bell tree to rank the best paths.


62 PRED: Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics We use the Pearson’s Spearman’s correlation coefficient to evaluate machine translation quality. We use the Stem set of the Adequacy Table to determine the correlation between the two variables. We use the Stem set of the Adequacy Table to determine the correlation between the two variables. We use the Stem set of the Adequacy Table to determine the correlation between the two variables. We use the Stem set of the Adequacy Table to determine the correlation between the two variables. We use the Stem set of the Adequacy Table to determine the correlation between the two variables.


63 PRED: Statistical Machine Translation By Parsing In an ordinary syntactic parser, the input is a string, and the grammar ranges over strings. This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples. Such algorithms can infer the synchronous structures hidden in parallel texts. It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system. A parser is an algorithm for inferring the structure of its input, guided by a grammar that dictates what structures are possible or probable. Such inference algorithms can perform various kinds of analysis on parallel texts, also known as multitexts. Figure 1 shows some of the ways in which ordinary parsing can be generalized. A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures. We use a synchronous parser to infer the structure of a multitext. We use a synchronous parser to infer the structure of a multitext.


64 PRED: Extracting Semantic Orientations Of Words Using Spin Model We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported. We use a spin model to extract semantic orientations of words from glosses (i.e., definition or explanation sentences) in a dictionary, as well as from a thesaurus and a corpus. We use a spin model to extract semantic orientations of words. We use a spin model to extract semantic orientations of words. We use a mean field approximation to compute the average orientation of each word. We use the mean field approximation to compute the average orientation of each word. We use a spin model to compute the average orientation of each word. We use a spin model to compute the average orientation of each word. We use a spin model to compute the average orientation of each word. We use a spin model to extract semantic orientation of words. We use a spin model to extract the average orientation of each word. We use a mean field approximation to compute the average orientation of each word. We use the mean field approximation to compute the average orientation of each word. We also propose a criterion for parameter selection on the basis of magnetization. We empirically show that the proposed method works well even with a small number of seed words. We use a spin model to extract semantic orientation of words. We use a spin model to extract semantic orientation of words. We use a spin model to compute the average orientation of each word. We use a mean field approximation to compute the average orientation of each word. We use a magnetization criterion for parameter selection on the basis of magnetization. We use a polarization criterion for parameter selection on the basis of magnetization. We use a lexical network of words.


65 PRED: Semantic Role Labeling Using Different Syntactic Views Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels. In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers. We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views. Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument. In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses. All of the reported techniques resulted in performance improvements. We show that the semantic role labeling problem is a significant bottleneck to improving overall system performance. We use a combination of features to improve the classification of semantic role tags.


66 PRED: Meaningful Clustering Of Senses Helps Boost Word Sense Disambiguation Performance Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task. We use the Oxford Dictionary of English senses to cluster words and use the Oxford Dictionary of English senses to reduce the number of clusterings. We use the Oxford Dictionary of English senses to reduce the number of clusterings. We use the Oxford Dictionary of English senses to reduce the number of clusterings. We use the Oxford Dictionary of English dictionary senses to reduce the number of clusterings.


67 PRED: Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning. We utilize meta-patterns of high-frequency words and content words in order to discover pattern candidates. Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets. Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words. We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNet-based evaluation. Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported. We use a clique-set graph algorithm to identify symmetric patterns among the candidates. We use a clique-set graph algorithm to identify a clique-set graph algorithm.


68 PRED: Tree-To-String Alignment Template For Statistical Machine Translation We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. We propose a tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string.


69 PRED: Contextual Dependencies In Unsupervised Word Segmentation Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech. We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively. The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation. We also show that previous probabilistic models rely crucially on suboptimal search procedures. We show that the n-gram segmentation model is more accurate than the unigram model. We use Gibbs sampling to sample the posterior distribution of possible segmentations. We use a Bayesian model to find utterance boundaries in a corpus of phonemically transcribed utterances.


70 PRED: Minimum Risk Annealing For Training Log-Linear Models 10 restarts 1 restart 793 Optimization Procedure labeled dependency accuracy. For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). We use a gaussian prior prior to minimize the error surface. We use a gaussian prior prior to minimize the error surface. We use a gaussian prior prior to minimize the error surface. We use a gaussian prior prior to minimize the error surface. We use a gaussian prior prior to minimize the error surface.


71 PRED: EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start) We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-of-the-art methods, while using simple and efficient learning methods. We use a Bayesian HMM model to compute the ambiguity level of the tagset and the ambiguity level of the tagset. We use a Bayesian HMM model to compute the ambiguity level of the tagset.


72 PRED: Co-Training for Cross-Lingual Sentiment Classification The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. We use a co-training approach to make full use of the two redundant views of features. The SVM classifier is adopted as the basic classifier in the proposed approach. We use a supervised approach to improve the classification accuracy of sentiment identification of Chinese product reviews.


73 PRED: A Gibbs Sampler for Phrasal Synchronous Grammar Induction We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchronous grammar from parallel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. We use a hierarchical Bayesian prior to impose a bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. We use a Gibbs sampler to perform inference over the latent synchronous derivation trees to impose a bias towards compact grammars with small translation units. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. We use a Gibbs sampler to induce a synchronous grammar from a word alignment tree.


74 PRED: A Latent Dirichlet Allocation Method for Selectional Preferences computation of preferthe admissible argument values for a relation, is a well-known NLP task with applicability. We present a system called LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, the benefits of previous approaches: like traditional class-based approaches, it produces human-interpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. Compare several state-of-the-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate effectiveness at filtering improper applications of inference rules, where we show substantial improvement Pantel system (Pantel et al., Selectional Preferences encode the set of admissible argument values for a relation. For example, locations are likely to appear in the second argument of the relation X is headquartered in Y and companies or organizations in the first. We use a latent Dirichlet allocation method (LDA) to model selectional preferences. We use a latent Dirichlet allocation method (LDA) to extract the information about the pairs of nouns that are commonly co-occur.


75 PRED: D-Theory: Talking About Talking About Trees Linguists, including computational linguists, have always been fond of talking about trees. In this paper, we outline a theory of linguistic structure which talks about talking about trees; we call this theory Description theory (D-theory). While important issues must be resolved before a complete picture of D-theory emerges (and also before we can build programs which utilize it), we believe that this theory will ultimately provide a framework for explaining the syntax and semantics of natural in a manner which is intrinsically computational. This paper will focus primarily on one set of motivations for this theory, those engendered by attempts to handle certain syntactic phenomena within the framework of deterministic parsing. We propose a D-theory model which uses a tree structure to describe the structure of a sentence.


76 PRED: Features And Values The paper discusses the linguistic aspects of a new general purpose facility for computing with features. The program was developed in connection with the course I taught at the University of Texas in the fall of 1983. It is a generalized and expanded version of a system that Stuart Shieber originally designed for the PATR-II project at SRI in the spring of 1983 with later modifications by Fernando Pereira and me. Like its predecessors, the new Texas version of the "DG (directed graph)" package is primarily intended for representing morphological and syntactic information but it may turn out to be very useful for semantic representation. We use angle brackets to mark expressions that designate paths. We use angle brackets to mark expressions that designate paths. We use a DG-based unification system to represent a DG-based unification system.


77 PRED: Functional Unification Grammar: A Formalism For Machine Translation This paper presents Functional Unification Grammar: A Formalism For Machine Translation. It is a formal approach to linguistic unification grammar that uses functional unification grammar to describe the linguistic objects in a machine translation system. The FUG framework is based on a formal unification grammar, which is a formalism for describing the linguistic objects in a machine translation system. The FUG framework is based on a formal unification grammar.


78 PRED: A Syntactic Approach To Discourse Semantics A correct structural analysis of a discourse is a prerequisite for understanding it. This paper sketches the outline of a discourse grammar which acknowledges several different levels of structure. This grammar, the "Dynamic Discourse Model", uses an Augmented Transition Network parsing mechanism to build a representation of the semantics of a discourse in a stepwise fashion, from left to right, on the basis of the semantic representations of the individual clauses which constitute the discourse. The intermediate states of the parser model the intermediate states of the social situation which generates the discourse. The paper attempts to demonstrate that a discourse may indeed be viewed as constructed by means of sequencing and recursive nesting of discourse constituents. It gives rather detailed examples of discourse structures at various levels, and shows how these structures are described in the framework proposed here. We propose a discourse grammar which uses a linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistically-advanced linguistic


79 PRED: Char Align: A Program For Aligning Parallel Texts At The Character Level There have been a number of recent papers on aligning parallel texts at the sentence level, e.g., Brown al and Church (to appear), Isabelle (1992), Kay and Rosenschein (to appear), Simard al Warwick— Armstrong and Russell (1990). On clean inputs, such as the Canadian Hansards, these methods have been very successful (at least 96% correct by sentence). Unfortunately, if the input is noisy (due to OCR and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find boundaries, let alone sentences. This paper describes a new program, aligns texts at the level rather than at the sentence/paragraph level, based on the cognate approach proposed by Simard al. We use char align to align parallel texts at the sentence level.


80 PRED: Principle-Based Parsing Without Overgeneration Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sentences. We propose a message passing algorithm for principle-based parsing that uses a unified grammar model to describe the relationships between nodes and links in the grammar network and their software counterparts. We propose a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sentences. We propose a message passing algorithm for principle-based parsing that avoids the overgeneration problem.


81 PRED: Distributional Clustering Of English Words We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data. We use a statistically similarity measure to measure the likelihood of a word in a given context.


82 PRED: Multi-Paragraph Segmentation Of Expository Text This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. We show that the lexical cohesion relations of the segmentation model is a very efficient segmentation model. We use a lexical frequency and distribution information model to segment the text into segments that reflect the subtopic structure of the texts. We use a lexical frequency and distribution information model to segment the text into segments.


83 PRED: Corpus Statistics Meet The Noun Compound: Some Empirical Results Tagged Dependency -.. Tagged Adjacency -e. L. Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results. Three training schemes have been used and the tuned analysis procedures applied to the test set. Figure 5 shows the resulting accuracy, with accuracy values from figure 3 displayed with dotted lines. If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy. However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%. We use a morphological analyser to find compound noun compounds in our corpus.


84 PRED: Unsupervised Word Sense Disambiguation Rivaling Supervised Methods This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure. We show that the SENSE-A/SENSE-B seed sets are a very large, completely untagged corpus. We use a supervised learning algorithm to automatically identify collocations for target senses of a word, given a few seed collocations. We use a supervised learning algorithm to automatically identify collocations for target senses of a word, given a few seed collocations.


85 PRED: Entity-Based Cross-Document Coreference Using the Vector Space Model Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task. In particular, resolving cross-document coreferences allows a user to identify trends and dependencies across documents. Cross-document coreference can also be used as the central tool for producing summaries from multiple documents, and for information fusion, both of which have been identified as advanced areas of research by the TIPSTER Phase III program. Cross-document coreference was also identified as one of the potential tasks for the Sixth Message Understanding Conference (MUC-6) but was not included as a formal task because it was considered too ambitious (Grishman 94). We use the Vector Space Model to resolve ambiguities between people having the same name. We use the Vector Space Model to resolve ambiguities between people having the same name.


86 PRED: Two Statistical Parsing Models Applied To The Chinese Treebank This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank. We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAG-based parsing model, adapted from (Chiang, 2000). On sentences with 40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall. We use a lexicalized PCFG parser to model Chinese treebank. We use a TAG-based parser to model Chinese tree-based parser.


87 PRED: Latent Semantic Analysis For Text Segmentation This paper describes a method for linear text segmentation that is more accurate or at least as accurate as state-of-the-art methods (Utiyama and Isahara, 2001; Choi, 2000a). Inter-sentence similarity is estimated by latent semantic analysis (LSA). Boundary locations are discovered by divisive clustering. Test results show LSA is a more accurate similarity measure than the cosine metric. We use a supervised clustering approach to extract a list of tokenised sentences into a list of words. We use a supervised clustering approach to extract the content words from the list. We use a supervised clustering approach to extract the content words from the list.


88 PRED: Building A Discourse-Tagged Corpus In The Framework Of Rhetorical Structure Theory The reliability of a dialogue structure coding Linguistics 13-32. Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language The advent of large-scale collections of annotated data has marked a paradigm shift in the research community for natural language processing. These corpora, now also common in many languages, have accelerated development efforts and energized the community. Annotation ranges from broad characterization of document-level information, such as topic or relevance judgments (Voorhees and Harman, 1999; Wayne, 2000) to discrete analysis of a wide range of linguistic phenomena. However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale. So far, the annotation of discourse structure of documents has been applied primarily to identifying topical segments (Hearst, 1997), inter-sentential relations (Nomoto and Matsumoto, 1999), and hierarchical analyses of small corpora (Moser and Moore, 1995), and we propose a discourse-tagged RST corpus. We propose a discourse-tagged RST corpus for NLP research.


89 PRED: NLTK: The Natural Language Toolkit NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset. We propose a streamlined and flexible way of organizing the practical component of an introductory computational linguistics course. We propose a set of modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware.


90 PRED: Unsupervised Discovery Of Morphemes We present two methods for unsupervised segmentation of words into morphemelike units. The model utilized is especially suited for languages with a rich morphology, such as Finnish. The first method is based on the Minimum Description Length (MDL) principle and works online. In the second method, Maximum Likelihood (ML) optimization is used. The quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis. Experiments on both Finnish and English corpora show that the presented methods perform well compared to a current state-of-the-art system. We propose a morphological segmentation method for Finnish, which uses a morphologically rich corpus.


91 PRED: Language Independent NER Using A Maximum Entropy Tagger Entity Recognition systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. We use a Gaussian prior to smooth out the large feature space. We use a POS tagger to extract a large number of overlapping features. We use a POS tagger to extract a large number of overlapping features.


92 PRED: Learning Extraction Patterns For Subjective Expressions This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm. The learned patterns are then used to identify more subjective sentences. The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision. We use a supervised extraction pattern learning algorithm to automatically identify subjective expressions from unannotated data. We use a supervised extraction pattern learning algorithm to automatically identify subjective expressions from unannotated texts. We use a supervised extraction pattern learning algorithm to automatically identify subjective expressions. The extracted extraction patterns are linguistically richer and more flexible than single words or N-grams. We use a supervised extraction pattern learning algorithm to automatically identify subjective expressions from unannotated data to automatically identify subjective expressions. We use a supervised extraction pattern learning algorithm to automatically identify subjective expressions from unannotated text.


93 PRED: HHMM-Based Chinese Lexical Analyzer ICTCLAS This document presents the results from Inst. of Computing Tech., CAS in the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff. The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks. Then provide the evaluation results and give more analysis. Evaluation on ICTCLAS shows that its performance is competitive. Compared with other system, ICTCLAS has ranked top both in CTB and PK closed track. In PK open track, it ranks second position. ICTCLAS BIG5 version was transformed from GB version only in two days; however, it achieved well in two BIG5 closed tracks. Through the first bakeoff, we could learn more about the development in Chinese word segmentation and become more confident on our HHMM-based approach. At the same time, we really find our problems during the evaluation. The bakeoff is interesting and helpful. We use a class-based approach to segment Chinese lexical analysis.


94 PRED: The Senseval-3 English Lexical Sample Task This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise. The task drew the participation of 27 teams from around the world, with a total of 47 systems. We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise. The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation. This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations. The main changes in this year's evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth). We use a “tag until two agree” scheme, with an upper bound on the number of senses collected for each item set to four. We use a “tag until two agree” scheme, with an upper bound on the number of senses collected for each word.


95 PRED: A Linear Programming Formulation For Global Inference In Natural Language Tasks Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the "human-like" quality of the inferences. We use a linear programming approach to learn global inference, which is a linear programming approach. We use a linear programming approach to learn global inference, which is a linear programming approach. We use a linear programming approach to learn global inference, which is a linear programming formulation for global inference.


96 PRED: Calibrating Features For Semantic Role Labeling This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed. We further show that different features are needed for different subtasks. Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the state-of-the-art in semantic analysis. We propose a semantic role labeling system that uses a maximum entropy classifier to classify the entropy classifier.


97 PRED: LexPageRank: Prestige In Multi-Document Text Summarization Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document. Centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank. In this model, a sentence connectivity matrix is constructed based on cosine similarity. If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix. We provide an evaluation of our method on DUC 2004 data. The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems. We use a feature-based generic summarization toolkit, MEAD, to measure sentence centrality. We use a graph-based prestige scoring method to measure sentence centrality.


98 PRED: Statistical Significance Tests For Machine Translation Evaluation If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU metric. Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real. We use a randomized method called bootstrap resampling to compute the true BLEU score. We use a randomized method called bootstrap resampling to compute the true BLEU score.


99 PRED: Domain Adaptation With Structural Correspondence Learning Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor domain. We introduce learning automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. We use unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains. We use structural correspondence learning (SCL). We use a structural correlation learning (SCL) to learn a common feature representation that is meaningful across both domains. We use a structural correlation learning (SCL) to learn a common feature representation that is meaningful across both domains. We use a structural correlation learning (SCL) to learn a common feature representation that is meaningful across both domains. We use a structural correlation learning (SCL) to learn a common feature representation that is meaningful across both domains.


100 PRED: Which Side Are You On? Identifying Perspectives At The Document And Sentence Levels In this paper we investigate a new problem identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy. We use subjective language to express opinions, emotions, and sentiments. We use subjective language to express the perspective of a document and use subjective language to express the perspective of a document.


101 PRED: METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments METEOR is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used metrics. It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German. We use a metric to measure the quality of a given translation.


102 PRED: Detection of Grammatical Errors Involving Prepositions This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. We use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. We use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3. We use a maximum entropy classifier to detect preposition errors in a corpus of student essays. Although this is work in progress, we achieve a precision of 0.8 with a recall of 0.3. We use a maximum entropy (ME) classifier to detect preposition errors in a corpus of ESL essays. We use a Bayesian classifier to detect preposition errors in a corpus of student essays. We use a maximum entropy (ME) classifier to detect preposition errors in a corpus of student essays. Although this is work in progress, we achieve a precision of 0.8 with a recall of 0.3. We use a maximum entropy (ME) classifier to detect preposition errors in a corpus of ESL essays. We use a Bayesian classifier to detect preposition errors in a corpus of English grammar test.


103 PRED: Overview of BioNLP&rsquo;09 Shared Task on Event Extraction The paper presents the design and implementation of the BioNLP'09 Shared Task, and reports the final results with analysis. The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. We present a system for a shared task that aims to extract bio-events from a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set of a set


104 PRED: Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon Even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper, we show how we create a high-quality, moderate-sized emotion lexicon using Mechanical Turk. In addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. We identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand. We use a large number of common words and phrases to generate a large number of common words and phrases to generate emotion lexicons.


105 PRED: Distinguishing Word Senses In Untagged Text This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text. The methods described in this paper, McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text. These methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs. Overall, the most accurate of these procedures is McQuitty's similarity analysis in combination with a high dimensional feature set. We use a supervised learning approach to disambiguate word senses in untagged text. We use McQuitty's similarity analysis, Ward's minimum-variance method, and the EM algorithm to distinguish between the known senses (i.e., as defined in some dictionary) of a word, based only on features that can be automatically extracted from untagged text. We use agglomerative clustering algorithms that form classes of unlabeled observations that minimize their respective distance measures between class members. We use a supervised learning approach to disambiguate 13 different words using 3 different feature sets. We use McQuitty's similarity analysis, McQuitty's minimum-variance method (Ward, 1963) and the EM algorithm to distinguish between the sense of an ambiguous word and a known sense definition. We use a supervised learning approach to cluster observations that minimize their respective distance measures between class members. We use a supervised learning approach to disambiguate a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word from a word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word based on the corresponding word


106 PRED: CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach. We propose a supervised coreference system that is capable of resolving a sub-class of anaphora that does not require general purpose reasoning. We propose a supervised coreference system that is capable of resolving a sub-class of anaphora that is capable of greater than 90% precision with 60% and better recall for some pronouns. We propose a supervised coreference system that is capable of resolving a sub-classifier that is capable of resolving a sub-classifier that is capable of resolving a sub-classifier that is capable of resolving a sub-classifier that is capable of resolving a sub-classifier that is a sub-classifier that is a sub-classifier that is a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub-classifier that is based on a sub


107 PRED: Edge-Based Best-First Chart Parsing Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged "best" by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finergrained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a facof twenty improvement over the best results - that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing. We use a supervised parser to parse the WSJ treebank and a supervised parser to parse the WSJ treebank.


108 PRED: Language Independent Named Entity Recognition Combining Morphological And Contextual Evidence Identifying and classifying personal, geographic, institutional or other names in a text is an important task for numerous applications. This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models. The algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools. We use a lexicon-based model to identify the most common names in a given language. We use a lexicon-based model to identify the most common names in a given language.


109 PRED: Detecting Text Similarity Over Short Passages: Exploring Linguistic Feature Combinations Via Machine Learning We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units. We use a combination of a combination of a TF*IDF metric to measure semantic distance between two small textual units (paragraph or sentence-sized) that contain common information, as a necessary step towards extracting such common information and constructing thematic groups of text units across multiple documents. We use a combination of TF*IDF to measure semantic distance between two similar text units.


